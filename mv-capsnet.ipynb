{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-11T16:12:35.052311Z",
     "iopub.status.busy": "2020-11-11T16:12:35.041749Z",
     "iopub.status.idle": "2020-11-11T16:12:40.273621Z",
     "shell.execute_reply": "2020-11-11T16:12:40.272763Z"
    },
    "papermill": {
     "duration": 5.257964,
     "end_time": "2020-11-11T16:12:40.273738",
     "exception": false,
     "start_time": "2020-11-11T16:12:35.015774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, \n",
    "not just on MNIST.\n",
    "*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\n",
    "uncommenting them and commenting their counterparts.\n",
    "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import initializers, layers\n",
    "\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + K.epsilon())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Length, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=x.shape[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix, from each input capsule to each output capsule, there's a unique weight as in Dense layer.\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule, 1]\n",
    "        inputs_expand = tf.expand_dims(tf.expand_dims(inputs, 1), -1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
    "        inputs_tiled = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
    "        # Regard the first two dimensions as `batch` dimension, then\n",
    "        # matmul(W, x): [..., dim_capsule, input_dim_capsule] x [..., input_dim_capsule, 1] -> [..., dim_capsule, 1].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = tf.squeeze(tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled))\n",
    "\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, 1, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[inputs.shape[0], self.num_capsule, 1, self.input_num_capsule])\n",
    "\n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, axis=1)\n",
    "\n",
    "            # c.shape = [batch_size, num_capsule, 1, input_num_capsule]\n",
    "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "            # The first two dimensions as `batch` dimension,\n",
    "            # then matmal: [..., 1, input_num_capsule] x [..., input_num_capsule, dim_capsule] -> [..., 1, dim_capsule].\n",
    "            # outputs.shape=[None, num_capsule, 1, dim_capsule]\n",
    "            outputs = squash(tf.matmul(c, inputs_hat))  # [None, 10, 1, 16]\n",
    "\n",
    "            if i < self.routings - 1:\n",
    "                # outputs.shape =  [None, num_capsule, 1, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension, then\n",
    "                # matmal:[..., 1, dim_capsule] x [..., input_num_capsule, dim_capsule]^T -> [..., 1, input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
    "                b += tf.matmul(outputs, inputs_hat, transpose_b=True)\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return tf.squeeze(outputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_capsule': self.num_capsule,\n",
    "            'dim_capsule': self.dim_capsule,\n",
    "            'routings': self.routings\n",
    "        }\n",
    "        base_config = super(CapsuleLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n",
    "\n",
    "class ViewPoolingLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, num_views, **kwargs):\n",
    "        super(ViewPoolingLayer, self).__init__(**kwargs)\n",
    "        self.num_views = num_views\n",
    "        self.tf_num_views = tf.constant(self.num_views)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.n_per_view = tf.divide(input_shape[0], self.tf_num_views)\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        reshaped = K.reshape(\n",
    "            inputs,\n",
    "            (self.n_per_view, self.tf_num_views, inputs.shape[-1])\n",
    "        )\n",
    "        result = K.max(reshaped, axis=1)\n",
    "        return tf.repeat(result, repeats=self.num_views, axis=0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ViewPoolingLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "class ViewCapsReshapeLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, num_caps, **kwargs):\n",
    "        super(ViewCapsReshapeLayer, self).__init__(**kwargs)\n",
    "        self.num_caps = num_caps\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.reshape(\n",
    "            inputs, (-1, self.num_caps, inputs.shape[-1])\n",
    "        )\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple(\n",
    "            int(input_shape[0] / self.num_caps),\n",
    "            self.num_caps, inputs.shape[-1]\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ViewCapsReshapeLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "    \n",
    "class ViewCapsRepeatLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, num_views, **kwargs):\n",
    "        super(ViewCapsRepeatLayer, self).__init__(**kwargs)\n",
    "        self.num_views = num_views\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.repeat(inputs, repeats=self.num_views, axis=0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple(\n",
    "            input_shape[0] * self.num_views,\n",
    "            inputs.shape[-2], inputs.shape[-1]\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ViewCapsRepeatLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "def ViewCapsuleLayer(inputs, num_views, dim_capsule, routings):\n",
    "    n_class = inputs.shape[1]\n",
    "    reshaped = ViewCapsReshapeLayer(\n",
    "        num_caps=num_views * n_class, name='viewcaps_reshape'\n",
    "    )(inputs)\n",
    "    view_caps = CapsuleLayer(\n",
    "        num_capsule=n_class, dim_capsule=dim_capsule, routings=routings, name='viewcaps'\n",
    "    )(reshaped)\n",
    "    return ViewCapsRepeatLayer(num_views=num_views, name='viewcaps_rep')(view_caps)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# The following is another way to implement primary capsule layer. This is much slower.\n",
    "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    outputs = []\n",
    "    for _ in range(n_channels):\n",
    "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
    "    outputs = layers.Concatenate(axis=1)(outputs)\n",
    "    return layers.Lambda(squash)(outputs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-11T16:12:40.449751Z",
     "iopub.status.busy": "2020-11-11T16:12:40.413294Z",
     "iopub.status.idle": "2020-11-11T16:12:47.286633Z",
     "shell.execute_reply": "2020-11-11T16:12:47.287224Z"
    },
    "papermill": {
     "duration": 7.007538,
     "end_time": "2020-11-11T16:12:47.287368",
     "exception": false,
     "start_time": "2020-11-11T16:12:40.279830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keras implementation of CapsNet in Hinton's paper Dynamic Routing Between Capsules.\n",
    "The current version maybe only works for TensorFlow backend. Actually it will be straightforward to re-write to TF code.\n",
    "Adopting to other backends should be easy, but I have not tested this. \n",
    "\n",
    "Usage:\n",
    "       python capsulenet.py\n",
    "       python capsulenet.py --epochs 50\n",
    "       python capsulenet.py --epochs 50 --routings 3\n",
    "       ... ...\n",
    "       \n",
    "Result:\n",
    "    Validation accuracy > 99.5% after 20 epochs. Converge to 99.66% after 50 epochs.\n",
    "    About 110 seconds per epoch on a single GTX1070 GPU card\n",
    "    \n",
    "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import os\n",
    "import argparse\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "import pandas as pd\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "WORK_DIR = './../input/views-rendered-from-3d-models/cleaned.dataset/'\n",
    "FILE_PATH_COLUMN = 'file_path'\n",
    "CLASS_COLUMN = 'label'\n",
    "# image_width, image_height = 224, 224\n",
    "# image_width, image_height = 30, 30\n",
    "image_width, image_height = 45, 45\n",
    "\n",
    "\n",
    "def CapsNet(input_shape, n_class, routings, batch_size):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param routings: number of routing iterations\n",
    "    :param batch_size: size of batch\n",
    "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
    "            `eval_model` can also be used for training.\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape, batch_size=batch_size)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    caps_layer1 = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings, name='caps1')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(caps_layer1)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,), name='recon_input')\n",
    "    masked_by_y = Mask(name='mask')([caps_layer1, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(caps_layer1)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(layers.Dense(512, activation='relu', input_dim=16 * n_class))\n",
    "    decoder.add(layers.Dense(1024, activation='relu'))\n",
    "    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    # Models for training and evaluation (prediction)\n",
    "    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "\n",
    "    return train_model, eval_model #, manipulate_model\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, 1))\n",
    "\n",
    "\n",
    "def train(model, train_set, test_set, args, model_type='sv'):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param train_set: data frame\n",
    "    :param test_set: data frame\n",
    "    :param args: arguments\n",
    "    :param model_type: argument\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger(args.save_dir + '/' + model_type + '-log.csv')\n",
    "    tb = callbacks.TensorBoard(\n",
    "        log_dir=args.save_dir + '/' + model_type + '-tensorboard-logs',\n",
    "        batch_size=args.batch_size, histogram_freq=int(args.debug)\n",
    "    )\n",
    "    checkpoint = callbacks.ModelCheckpoint(\n",
    "        args.save_dir + '/' + model_type + '-weights-{epoch:02d}.h5',\n",
    "        monitor='val_capsnet_acc',\n",
    "        save_best_only=True, save_weights_only=True, verbose=1\n",
    "    )\n",
    "#     lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: args.lr * (args.lr_decay ** epoch))\n",
    "    \n",
    "    output_layer_name = 'capsnet'\n",
    "    if model_type == 'mv' and args.mv_function == ViewPoolingLayer:\n",
    "        output_layer_name = 'mv_capsnet'\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(lr=args.lr),\n",
    "        loss=[margin_loss, 'mse'],\n",
    "        loss_weights=[1., args.lam_recon],\n",
    "        metrics={output_layer_name: 'accuracy'}\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    \"\"\"\n",
    "\n",
    "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "    def create_generator(df, batch_size, model_type='sv', n_views=None):\n",
    "        data_gen = ImageDataGenerator(rescale=1. / 255)\n",
    "        if model_type == 'mv':\n",
    "            df = df.sort_values(FILE_PATH_COLUMN)\n",
    "            random_order = []\n",
    "            for obj_index in np.random.permutation(int(df.shape[0] / n_views)):\n",
    "                random_order += range(obj_index * n_views, (obj_index + 1) * n_views)\n",
    "            df['order'] = random_order\n",
    "            df = df.sort_values('order').drop('order', axis='columns')\n",
    "        generator = data_gen.flow_from_dataframe(\n",
    "            df, directory=WORK_DIR, x_col=FILE_PATH_COLUMN, y_col=CLASS_COLUMN,\n",
    "            target_size=(image_width, image_height), batch_size=batch_size,\n",
    "            shuffle=model_type != 'mv'\n",
    "        )\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    model.fit_generator(\n",
    "        create_generator(\n",
    "            train_set, args.batch_size, model_type, args.multi_view\n",
    "        ),\n",
    "        steps_per_epoch=int(len(train_set) / args.batch_size),\n",
    "        epochs=args.epochs,\n",
    "        validation_data=create_generator(\n",
    "            test_set, args.batch_size, model_type, args.multi_view\n",
    "        ),\n",
    "        validation_steps=int(test_set.shape[0] / args.batch_size),\n",
    "        callbacks=[log, tb, checkpoint]\n",
    "    )\n",
    "\n",
    "    model.save_weights(args.save_dir + '/' + model_type + '-trained_model.h5')\n",
    "    print(\n",
    "        'Trained model saved to \\'%s/%s-trained_model.h5\\'' % (args.save_dir, model_type)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def add_view_pooling(model, args):\n",
    "    capsnet = [layer for layer in model.layers if layer.name == 'capsnet'][0].output\n",
    "    view_pooling = ViewPoolingLayer(name='mv_capsnet', num_views=args.multi_view)(capsnet)\n",
    "    \n",
    "    decoder = [layer for layer in model.layers if layer.name == 'decoder'][0]\n",
    "    mask = [layer for layer in model.layers if layer.name == 'mask'][0].output\n",
    "    return models.Model(model.inputs, [view_pooling, decoder(mask)])\n",
    "\n",
    "\n",
    "def add_view_capsule(model, args):\n",
    "    caps1 = [layer for layer in model.layers if layer.name == 'caps1'][0].output\n",
    "    view_caps = ViewCapsuleLayer(caps1, args.multi_view, 32, args.routings)\n",
    "    capsnet = [layer for layer in model.layers if layer.name == 'capsnet'][0](view_caps)\n",
    "    \n",
    "    decoder = [layer for layer in model.layers if layer.name == 'decoder'][0]\n",
    "    mask = [layer for layer in model.layers if layer.name == 'mask'][0].output\n",
    "    return models.Model(model.inputs, [capsnet, decoder(mask)])\n",
    "\n",
    "\n",
    "def test(model, data):\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    total_tp = 0\n",
    "    label_no = 0\n",
    "    for label in test_set[CLASS_COLUMN].unique():\n",
    "        label_data = data.query(f'{CLASS_COLUMN} == \"{label}\"')\n",
    "        test_generator = test_datagen.flow_from_dataframe(\n",
    "            label_data, directory=WORK_DIR, x_col=FILE_PATH_COLUMN, y_col=CLASS_COLUMN,\n",
    "            target_size=(image_width, image_height)\n",
    "        )\n",
    "        nb_samples = len(test_generator.filenames)\n",
    "        y_pred, recon = model.predict_generator(test_generator, steps=nb_samples)\n",
    "        tp = np.sum(np.argmax(np.unique(y_pred, axis=0), 1) == label_no)\n",
    "        print(f'Class {label} acc: {tp / label_data.shape[0]}')\n",
    "\n",
    "        total_tp += tp\n",
    "        label_no += 1\n",
    "    print('Test acc:', total_tp / data.shape[0])\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    classes = [\n",
    "        class_name for class_name in os.listdir(WORK_DIR)\n",
    "        if os.path.isdir(WORK_DIR + class_name)\n",
    "    ]\n",
    "    for class_name in classes:\n",
    "        for file in os.listdir(WORK_DIR + class_name + '/train'):\n",
    "            train_set.append((class_name, class_name + '/train/' + file))\n",
    "        for file in os.listdir(WORK_DIR + class_name + '/test'):\n",
    "            test_set.append((class_name, class_name + '/test/' + file))\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(train_set, columns=[CLASS_COLUMN, FILE_PATH_COLUMN]),\n",
    "        pd.DataFrame(test_set, columns=[CLASS_COLUMN, FILE_PATH_COLUMN])\n",
    "    )\n",
    "\n",
    "(train_set, test_set) = load_data()\n",
    "\n",
    "train_set = train_set.assign(\n",
    "    to_drop = lambda x: x[FILE_PATH_COLUMN].str.contains(\n",
    "        r'0565|0357|0536'\n",
    "    )\n",
    ") \\\n",
    "    .query(f'{CLASS_COLUMN} != \"chair\" or not to_drop') \\\n",
    "    .drop('to_drop', axis = 1)\n",
    "test_set = test_set.assign(\n",
    "    to_drop = lambda x: x[FILE_PATH_COLUMN].str.contains(\n",
    "        r'chair_0905|car_0242|guitar_0205|airplane_0669.'\n",
    "    )\n",
    ") \\\n",
    "    .query('not to_drop').drop('to_drop', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T16:12:47.501759Z",
     "iopub.status.busy": "2020-11-11T16:12:47.309260Z",
     "iopub.status.idle": "2020-11-11T22:42:31.217079Z",
     "shell.execute_reply": "2020-11-11T22:42:31.216403Z"
    },
    "papermill": {
     "duration": 23383.922569,
     "end_time": "2020-11-11T22:42:31.217249",
     "exception": false,
     "start_time": "2020-11-11T16:12:47.294680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    epochs = 5\n",
    "    batch_size = 8 * 12\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.9\n",
    "    lam_recon = 0.392\n",
    "    routings = 3\n",
    "    shift_fraction = 0.\n",
    "    debug = False\n",
    "    save_dir = './experiment'\n",
    "    testing = False\n",
    "    weights = None\n",
    "    multi_view = 12\n",
    "    mv_function = add_view_capsule\n",
    "\ttwo_stage = True\n",
    "    \n",
    "print(vars(args))\n",
    "\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "model, eval_model = CapsNet(\n",
    "    input_shape=(image_width, image_height, 3),\n",
    "    n_class=len(train_set[CLASS_COLUMN].unique()),\n",
    "    routings=args.routings,\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "if args.weights is not None:  # init the model weights with provided one\n",
    "    eval_model.load_weights(args.weights)\n",
    "    model.load_weights(args.weights)\n",
    "if not args.testing:\n",
    "\tif two_stage:\n",
    "\t\tmodel = train(model=model, train_set=train_set, test_set=test_set, args=args)\n",
    "\tif args.multi_view:\n",
    "\t\tmodel = args.mv_function(model, args)\n",
    "\t\tmodel.summary()\n",
    "\t\ttrain(\n",
    "\t\t\tmodel=model, train_set=train_set, test_set=test_set,\n",
    "\t\t\targs=args, model_type='mv'\n",
    "\t\t)\n",
    "\t\t\n",
    "else:  # as long as weights are given, will run testing\n",
    "    if args.weights is None:\n",
    "        print('No weights are provided. Will test using random initialized weights.')\n",
    "    test(model=eval_model, data=test_set)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "duration": 23417.970372,
   "end_time": "2020-11-11T22:42:48.494270",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-11T16:12:30.523898",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
